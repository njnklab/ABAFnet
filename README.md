**[English](README.md)** | **[‰∏≠Êñá](README_CN.md)**

# ABAFnet
Attention-Based Acoustic Feature Fusion Network for Depression Detection

<img src="https://github.com/xuxiaoooo/ABAFnet/blob/main/draw/fig2.jpg" width="500" height="500" alt="Backbone Flow"/><br/>

---

## üìô Dataset Application

Our team will work on multi-population, multi-modal big data set acquisition and artificial intelligence methods.

The public request of data will be firstly obtained in all the related repositories of our team (most of them are not licensed for public use at the moment due to the privacy policy)

**Note:** If you have questions about the article as a reviewer, please contact the corresponding author to ask for the feature data files.

For audio/video related data and research, please stay tuned to this code repository, and for brain imaging, EEG, physiological signaling data, please follow corresponding author Xizhe Zhang's subsequent papers.

---

## üìå Introduction

This repository contains the implementation of our proposed **ABAFnet**, a novel integrated speech feature attention-based recurrent network for efficient depression detection and analysis. The main goal of this project is to provide an accurate and efficient approach for detecting depression using speech data.

_**Note:** The code we released is not compelete in some detail due to the data privacy, but the model structure parts are Complete Edition. For our concern about data privacy, we have hidden the part about the data, please contact us by email if you need._

---

## üí° Features
- Feature extraction from speech data.
- Attention mechanism for better feature representation.
- Recurrent network architecture for modeling temporal information.
- Efficient and accurate depression detection.

---

## üõ†Ô∏è Installation and Usage
**Clone the repository**
```bash
git clone https://github.com/xuxiaoooo/ABAFnet.git
cd ABAFnet
```
---

## üìä Results

Our proposed ABAFnet achieved state-of-the-art performance in speech-based depression detection task with multi features. Detailed results and comparison with other methods can be found in our paper.

---

## üìÑ Citation
Paper Link: 
Arxiv: https://arxiv.org/pdf/2308.12478v1.pdf


Research Gate: https://www.researchgate.net/publication/373364067_Attention-Based_Acoustic_Feature_Fusion_Network_for_Depression_Detection#fullTextFileContent

If you find this work helpful, please cite our paper:
```
@misc{xu2023attentionbased,
      title={Attention-Based Acoustic Feature Fusion Network for Depression Detection}, 
      author={Xiao Xu and Yang Wang and Xinru Wei and Fei Wang and Xizhe Zhang},
      year={2023},
      eprint={2308.12478},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}
```

---

## üìß Contact

For any questions, feel free to open an issue or contact us at xuxiaooo1111@gmail.com

---
